{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d5669b-b8c9-43c3-9931-5f95ff18782e",
   "metadata": {},
   "source": [
    "# Ejercicios Dataframes 2\n",
    "Realiza los siguientes ejercicios. Soluciona cada uno de ellos empleando la DataFrame API y Spark SQL\n",
    "1. Inicializa la variable spark (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b159a865-1442-4d35-a8dd-056a550455b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "import string\n",
    "import sys\n",
    "from pyspark import sql\n",
    "\n",
    "spark = sql.SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"04-dfej2\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"hdfs:///spark/logs/history\") \\\n",
    "    .config(\"spark.history.fs.logDirectory\", \"hdfs:///spark/logs/history\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "#SQL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857de07-bca6-4560-8980-63f34adcec18",
   "metadata": {},
   "source": [
    "2. Crea un dataframe llamado mi_df con los datos del archivo *data/retail-data/all/online-retail-dataset.csv* (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8406cbab-5e1d-4b65-a2ec-436422a996c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "# Definir el esquema\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", StringType(), True),  # ou TimestampType se se parsea\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leer el archivo CSV con el esquema especificado\n",
    "\n",
    "mi_df = spark.read.csv(\"hdfs:///user/jovyan/data/retail-data/all/online-retail-dataset.csv\", header=True, schema=schema)\n",
    "#SQL\n",
    "mi_df.createOrReplaceTempView(\"retail\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8acc9b-b1d0-46db-9bcc-6e290827d05c",
   "metadata": {},
   "source": [
    "3. Cuenta el número de celdas totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3065e4f9-43d1-4e62-a0e3-113b5afe566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "#DataFrame API\n",
    "mi_df.select(count(\"*\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"Select count(*) from retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c134111-a5cb-473d-bee7-c15a4d0e5d46",
   "metadata": {},
   "source": [
    "4. Cuenta el número de \"invoiceNo\" distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c2da2f-911b-46cd-92b3-e15314dd7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT InvoiceNo)|\n",
      "+-------------------------+\n",
      "|                    25900|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT InvoiceNo)|\n",
      "+-------------------------+\n",
      "|                    25900|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "#DataFrame API\n",
    "mi_df.select(countDistinct(\"InvoiceNo\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"select count(distinct InvoiceNo) from retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69047d7e-2a4f-4536-b238-994e4d8a5686",
   "metadata": {},
   "source": [
    "5. Obtén el número de factura más bajo y el más alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dc87d54-2b9e-44ae-8ccc-de8b1c687f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|factura_min|factura_max|\n",
      "+-----------+-----------+\n",
      "|     536365|    C581569|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+-----------+\n",
      "|factura_min|factura_max|\n",
      "+-----------+-----------+\n",
      "|     536365|    C581569|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.functions import min, max\n",
    "mi_df.select(\n",
    "    min(\"InvoiceNo\").alias(\"factura_min\"),\n",
    "    max(\"InvoiceNo\").alias(\"factura_max\")\n",
    ").show()\n",
    "#SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        MIN(InvoiceNo) AS factura_min,\n",
    "        MAX(InvoiceNo) AS factura_max\n",
    "    FROM retail\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e648e-e749-4836-83b4-cac727e3b4d8",
   "metadata": {},
   "source": [
    "6. Obtén la suma de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "685348d8-f9cb-4f3b-a44b-437d7816b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             suma|\n",
      "+-----------------+\n",
      "|2245715.473997657|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|             suma|\n",
      "+-----------------+\n",
      "|2245715.473997657|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "mi_df.filter(\"Country == 'United Kingdom'\") \\\n",
    "  .select(sum(\"UnitPrice\").alias(\"suma\")) \\\n",
    "  .show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"Select sum (UnitPrice) as suma FROM retail where Country = 'United Kingdom' \").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f81fcd-0908-4ddd-a075-432aafbfb427",
   "metadata": {},
   "source": [
    "7. Obtén la media de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba9fc37-19e4-469f-968c-c238a7e055bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            media|\n",
      "+-----------------+\n",
      "|4.532422174138221|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|            media|\n",
      "+-----------------+\n",
      "|4.532422174138221|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "mi_df.filter(\"Country == 'United Kingdom'\") \\\n",
    "  .select(avg(\"UnitPrice\").alias(\"media\")) \\\n",
    "  .show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"Select avg (UnitPrice) as media FROM retail where Country = 'United Kingdom' \").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065cca6-b302-4f60-9a45-8d841bea0611",
   "metadata": {},
   "source": [
    "8. Obtén la el número total de productos vendidos (quantity) agrupado por países (muestra los 10 primeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbedc383-4684-4014-b3c7-48e4d6296cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|        Country|sum(Quantity)|\n",
      "+---------------+-------------+\n",
      "|         Sweden|        35637|\n",
      "|      Singapore|         5234|\n",
      "|        Germany|       117448|\n",
      "|            RSA|          352|\n",
      "|         France|       110480|\n",
      "|         Greece|         1556|\n",
      "|        Belgium|        23152|\n",
      "|        Finland|        10666|\n",
      "|          Malta|          944|\n",
      "|    Unspecified|         3300|\n",
      "|          Italy|         7999|\n",
      "|           EIRE|       142637|\n",
      "|         Norway|        19247|\n",
      "|          Spain|        26824|\n",
      "|        Denmark|         8188|\n",
      "|      Hong Kong|         4769|\n",
      "|         Israel|         4353|\n",
      "|        Iceland|         2458|\n",
      "|Channel Islands|         9479|\n",
      "|            USA|         1034|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-------------+\n",
      "|        Country|sum(Quantity)|\n",
      "+---------------+-------------+\n",
      "|         Sweden|        35637|\n",
      "|      Singapore|         5234|\n",
      "|        Germany|       117448|\n",
      "|            RSA|          352|\n",
      "|         France|       110480|\n",
      "|         Greece|         1556|\n",
      "|        Belgium|        23152|\n",
      "|        Finland|        10666|\n",
      "|          Malta|          944|\n",
      "|    Unspecified|         3300|\n",
      "|          Italy|         7999|\n",
      "|           EIRE|       142637|\n",
      "|         Norway|        19247|\n",
      "|          Spain|        26824|\n",
      "|        Denmark|         8188|\n",
      "|      Hong Kong|         4769|\n",
      "|         Israel|         4353|\n",
      "|        Iceland|         2458|\n",
      "|Channel Islands|         9479|\n",
      "|            USA|         1034|\n",
      "+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "\n",
    "mi_df.groupBy(\"Country\").sum(\"Quantity\").show()\n",
    "#SQL\n",
    "spark.sql(\"SELECT Country, sum(Quantity) FROM retail GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafc7bc-922f-46a2-a1cb-7c5c78b1014f",
   "metadata": {},
   "source": [
    "9. Obtén la media de los precios unitarios de los productos vendidos agrupada por países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1ede03e-6e12-4b14-804a-5deac392647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|        Country|    avg(UnitPrice)|\n",
      "+---------------+------------------+\n",
      "|         Sweden| 3.910887445887447|\n",
      "|      Singapore|109.64580786026204|\n",
      "|        Germany| 3.966929963138558|\n",
      "|            RSA| 4.277586206896552|\n",
      "|         France| 5.028864087881328|\n",
      "|         Greece| 4.885547945205478|\n",
      "|        Belgium| 3.644335427742861|\n",
      "|        Finland|  5.44870503597123|\n",
      "|          Malta| 5.244173228346455|\n",
      "|    Unspecified| 2.699573991031391|\n",
      "|          Italy| 4.831120797011214|\n",
      "|           EIRE| 5.911077354807337|\n",
      "|         Norway| 6.012025782688754|\n",
      "|          Spain| 4.987544413738618|\n",
      "|        Denmark|3.2569408740359873|\n",
      "|      Hong Kong| 42.50520833333331|\n",
      "|         Israel| 3.633131313131315|\n",
      "|        Iceland|2.6440109890109893|\n",
      "|Channel Islands| 4.932124010554092|\n",
      "|            USA|2.2164261168384876|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+------------------+\n",
      "|        Country|    avg(UnitPrice)|\n",
      "+---------------+------------------+\n",
      "|         Sweden| 3.910887445887447|\n",
      "|      Singapore|109.64580786026204|\n",
      "|        Germany| 3.966929963138558|\n",
      "|            RSA| 4.277586206896552|\n",
      "|         France| 5.028864087881328|\n",
      "|         Greece| 4.885547945205478|\n",
      "|        Belgium| 3.644335427742861|\n",
      "|        Finland|  5.44870503597123|\n",
      "|          Malta| 5.244173228346455|\n",
      "|    Unspecified| 2.699573991031391|\n",
      "|          Italy| 4.831120797011214|\n",
      "|           EIRE| 5.911077354807337|\n",
      "|         Norway| 6.012025782688754|\n",
      "|          Spain| 4.987544413738618|\n",
      "|        Denmark|3.2569408740359873|\n",
      "|      Hong Kong| 42.50520833333331|\n",
      "|         Israel| 3.633131313131315|\n",
      "|        Iceland|2.6440109890109893|\n",
      "|Channel Islands| 4.932124010554092|\n",
      "|            USA|2.2164261168384876|\n",
      "+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "mi_df.groupBy(\"Country\").avg(\"UnitPrice\").show()\n",
    "\n",
    "#SQL\n",
    "\n",
    "spark.sql(\"SELECT Country, avg(UnitPrice) FROM retail GROUP BY Country\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62faa914-1b71-4051-b2e3-e1e1454adada",
   "metadata": {},
   "source": [
    "10. Obtén el importe total (quantity * unit price) agrupado por número de factura (invoiceNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "43ef1acd-345e-4907-a5ab-107a408b999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|InvoiceNo|        sum(total)|\n",
      "+---------+------------------+\n",
      "|   563020| 605.1400000000001|\n",
      "|   565747|315.65000000000003|\n",
      "|   566248|            140.96|\n",
      "|   566431|            303.36|\n",
      "|   567163|305.06999999999994|\n",
      "|   567695|               0.0|\n",
      "|   567879| 534.2799999999997|\n",
      "|   568222|            185.85|\n",
      "|   568711|            112.32|\n",
      "|   569020|1238.5400000000004|\n",
      "|   569560|318.44000000000005|\n",
      "|   569823|219.13999999999996|\n",
      "|   570234|328.53000000000003|\n",
      "|   570264|               0.0|\n",
      "|   570281| 676.3199999999999|\n",
      "|   570592|           2299.61|\n",
      "|   571010|               0.0|\n",
      "|   571906|14.850000000000001|\n",
      "|   572049| 414.2100000000001|\n",
      "|   572458|            498.74|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------------------------+\n",
      "|InvoiceNo|sum((quantity * UnitPrice))|\n",
      "+---------+---------------------------+\n",
      "|   563020|          605.1400000000001|\n",
      "|   565747|         315.65000000000003|\n",
      "|   566248|                     140.96|\n",
      "|   566431|                     303.36|\n",
      "|   567163|         305.06999999999994|\n",
      "|   567695|                        0.0|\n",
      "|   567879|          534.2799999999997|\n",
      "|   568222|                     185.85|\n",
      "|   568711|                     112.32|\n",
      "|   569020|         1238.5400000000004|\n",
      "|   569560|         318.44000000000005|\n",
      "|   569823|         219.13999999999996|\n",
      "|   570234|         328.53000000000003|\n",
      "|   570264|                        0.0|\n",
      "|   570281|          676.3199999999999|\n",
      "|   570592|                    2299.61|\n",
      "|   571010|                        0.0|\n",
      "|   571906|         14.850000000000001|\n",
      "|   572049|          414.2100000000001|\n",
      "|   572458|                     498.74|\n",
      "+---------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "#DataFrame API\n",
    "mi_df.withColumn(\"total\", expr(\"quantity * UnitPrice\")).groupBy(\"InvoiceNo\").sum(\"total\").show()\n",
    "#SQL\n",
    "spark.sql(\"SELECT InvoiceNo, sum( quantity*UnitPrice)  FROM retail GROUP BY InvoiceNo\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125443d3-7d2c-4cec-b97e-181a87ef4574",
   "metadata": {},
   "source": [
    "11. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. (sólo mostrar los alumnos que tengan nota en las 3 asignaturas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2110e4ec-d5e7-4289-98ad-414a8880b0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "from pyspark.sql.functions import col\n",
    "notas_fisica = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"hdfs:/user/jovyan/data/notas/notas_fisica.txt\").toDF(\"alumno\",\"nota_fisica\")\n",
    "notas_ingles = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"hdfs:/user/jovyan/data/notas/notas_ingles.txt\").toDF(\"alumno\",\"nota_ingles\")\n",
    "notas_matematicas = spark.read.option(\"header\",False).option(\"inferSchema\",True).csv(\"hdfs:/user/jovyan/data/notas/notas_mates.txt\").toDF(\"alumno\",\"nota_matematicas\")\n",
    "\n",
    "\n",
    "notas_join = notas_fisica.join(notas_matematicas, \"alumno\", \"inner\").join(notas_ingles, \"alumno\", \"inner\")\n",
    "notas_join.show()\n",
    "#.join(notas_ingles)\n",
    "#notas_join.show()\n",
    "#SQL\n",
    "notas_fisica.createOrReplaceTempView(\"fisica\")\n",
    "notas_matematicas.createOrReplaceTempView(\"matematicas\")\n",
    "notas_ingles.createOrReplaceTempView(\"ingles\")\n",
    "spark.sql(\"SELECT fisica.alumno, nota_fisica, nota_matematicas, nota_ingles FROM fisica, matematicas, ingles WHERE fisica.alumno = matematicas.alumno and matematicas.alumno = ingles.alumno\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68b0a9-90b3-408a-be73-80ed93ab913d",
   "metadata": {},
   "source": [
    "12. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. Si a un alumno le falta alguna de las notas aparecerá el valor NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f88c5b68-5482-4035-a5a2-d08a30c11776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|    Pedro|          2|             5.0|       NULL|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n",
      "+---------+-----------+----------------+-----------+\n",
      "|   alumno|nota_fisica|nota_matematicas|nota_ingles|\n",
      "+---------+-----------+----------------+-----------+\n",
      "|Alejandro|          3|             5.0|          7|\n",
      "|   Anabel|          2|             8.0|          7|\n",
      "|   Andres|          4|             4.0|          6|\n",
      "|    Angel|          9|             6.0|          4|\n",
      "|   Carlos|          4|             4.0|          8|\n",
      "| Fernando|          9|             5.0|          7|\n",
      "|   Isabel|          8|             8.0|          7|\n",
      "|    Jorge|          5|            10.0|          5|\n",
      "|Jose Juan|          3|             5.0|          3|\n",
      "| Leonardo|          6|             1.0|          4|\n",
      "|    Maria|          3|             2.0|          6|\n",
      "|  Nicolas|          7|             2.0|          5|\n",
      "|    Oscar|          5|             7.0|          3|\n",
      "|    Pedro|          2|             5.0|       NULL|\n",
      "|    Ramon|          7|             4.5|          8|\n",
      "|    Rocio|          5|             6.0|          4|\n",
      "|    Rocio|          7|             6.0|          4|\n",
      "|     Rosa|          8|             6.0|          9|\n",
      "|   Susana|          9|             9.0|          2|\n",
      "|   Triana|          3|             3.0|          4|\n",
      "+---------+-----------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "notas_outer_join = notas_fisica.join(notas_matematicas, \"alumno\", \"full_outer\").join(notas_ingles, \"alumno\", \"full_outer\")\n",
    "notas_outer_join.show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT fisica.alumno, nota_fisica, nota_matematicas, nota_ingles FROM fisica FULL OUTER JOIN matematicas ON fisica.alumno = matematicas.alumno FULL OUTER JOIN ingles ON coalesce(fisica.alumno, matematicas.alumno) = ingles.alumno\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690eca1-8683-45d0-95bf-78596f103a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb2bc470-d95c-4b6b-91dc-2fd7c3f56b16",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
