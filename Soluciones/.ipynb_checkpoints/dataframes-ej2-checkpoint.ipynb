{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d5669b-b8c9-43c3-9931-5f95ff18782e",
   "metadata": {},
   "source": [
    "# Ejercicios Dataframes 2\n",
    "Realiza los siguientes ejercicios. Soluciona cada uno de ellos empleando la DataFrame API y Spark SQL\n",
    "1. Inicializa la variable spark (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b159a865-1442-4d35-a8dd-056a550455b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "#Hay que usar agregaciones, entre otras cosas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857de07-bca6-4560-8980-63f34adcec18",
   "metadata": {},
   "source": [
    "2. Crea un dataframe llamado mi_df con los datos del archivo *data/retail-data/all/online-retail-dataset.csv* (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8406cbab-5e1d-4b65-a2ec-436422a996c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "mi_df = spark.read.option(\"header\", True).csv('data/all/online-retail-dataset.csv') #Si tiene un header, hay que poner option(\"header\", True) o si no se come los headers y se los inventa\n",
    "mi_df.show()\n",
    "#SQL\n",
    "mi_df.createOrReplaceTempView(\"retail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8acc9b-b1d0-46db-9bcc-6e290827d05c",
   "metadata": {},
   "source": [
    "3. Cuenta el número de celdas totales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3065e4f9-43d1-4e62-a0e3-113b5afe566a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "+----------------+\n",
      "|NumCeldasTotales|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "#DataFrame API\n",
    "mi_df.select(count(\"*\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT COUNT(*) AS NumCeldasTotales FROM retail\").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c134111-a5cb-473d-bee7-c15a4d0e5d46",
   "metadata": {},
   "source": [
    "4. Cuenta el número de \"invoiceNo\" distintos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c2da2f-911b-46cd-92b3-e15314dd7b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT invoiceNo)|\n",
      "+-------------------------+\n",
      "|                    25900|\n",
      "+-------------------------+\n",
      "\n",
      "+-----------+\n",
      "|NumInvoices|\n",
      "+-----------+\n",
      "|      25900|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "#DataFrame API\n",
    "mi_df.select(countDistinct(\"invoiceNo\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT COUNT(DISTINCT invoiceNo) AS NumInvoices FROM retail\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69047d7e-2a4f-4536-b238-994e4d8a5686",
   "metadata": {},
   "source": [
    "5. Obtén el número de factura más bajo y el más alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dc87d54-2b9e-44ae-8ccc-de8b1c687f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|max(invoiceNo)|min(invoiceNo)|\n",
      "+--------------+--------------+\n",
      "|       C581569|        536365|\n",
      "+--------------+--------------+\n",
      "\n",
      "+-------------+-------------+\n",
      "|NumFacturaMin|NumFacturaMax|\n",
      "+-------------+-------------+\n",
      "|       536365|      C581569|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max,min\n",
    "#DataFrame API\n",
    "mi_df.select(max(\"invoiceNo\"),min(\"invoiceNo\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT MIN(InvoiceNo) AS NumFacturaMin, MAX(InvoiceNo) AS NumFacturaMax FROM retail\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e648e-e749-4836-83b4-cac727e3b4d8",
   "metadata": {},
   "source": [
    "6. Obtén la suma de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "685348d8-f9cb-4f3b-a44b-437d7816b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|SumaImportesUnitarios|\n",
      "+---------------------+\n",
      "|    2245715.474000284|\n",
      "+---------------------+\n",
      "\n",
      "+-----------------+\n",
      "|         TotalSum|\n",
      "+-----------------+\n",
      "|2245715.474000284|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, sum_distinct, col\n",
    "#DataFrame API\n",
    "#mi_df.select(sum(\"UnitPrice\")).filter(\"Country = 'United Kingdom'\")\n",
    "mi_df.filter(col(\"Country\") == \"United Kingdom\").agg(sum(\"UnitPrice\").alias(\"SumaImportesUnitarios\")).show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT SUM(UnitPrice) AS TotalSum FROM retail WHERE Country = 'United Kingdom'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f81fcd-0908-4ddd-a075-432aafbfb427",
   "metadata": {},
   "source": [
    "7. Obtén la media de todos los importes unitarios de los productos vendidos en el Reino Unido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba9fc37-19e4-469f-968c-c238a7e055bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|MediaImportesUnitarios|\n",
      "+----------------------+\n",
      "|     4.532422174143522|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|MediaImportesUnitarios|\n",
      "+----------------------+\n",
      "|     4.532422174143522|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col\n",
    "#DataFrame API\n",
    "media_importes_unitarios_df = mi_df.filter(col(\"Country\") == \"United Kingdom\").agg(avg(\"UnitPrice\").alias(\"MediaImportesUnitarios\"))\n",
    "media_importes_unitarios_df.show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT AVG(UnitPrice) AS MediaImportesUnitarios FROM retail WHERE Country = 'United Kingdom'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065cca6-b302-4f60-9a45-8d841bea0611",
   "metadata": {},
   "source": [
    "8. Obtén la el número total de productos vendidos (quantity) agrupado por países (muestra los 10 primeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbedc383-4684-4014-b3c7-48e4d6296cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+\n",
      "|       Country|TotalProductosVendidos|\n",
      "+--------------+----------------------+\n",
      "|United Kingdom|             4263829.0|\n",
      "|   Netherlands|              200128.0|\n",
      "|          EIRE|              142637.0|\n",
      "|       Germany|              117448.0|\n",
      "|        France|              110480.0|\n",
      "|     Australia|               83653.0|\n",
      "|        Sweden|               35637.0|\n",
      "|   Switzerland|               30325.0|\n",
      "|         Spain|               26824.0|\n",
      "|         Japan|               25218.0|\n",
      "+--------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------+----------------------+\n",
      "|       Country|TotalProductosVendidos|\n",
      "+--------------+----------------------+\n",
      "|United Kingdom|             4263829.0|\n",
      "|   Netherlands|              200128.0|\n",
      "|          EIRE|              142637.0|\n",
      "|       Germany|              117448.0|\n",
      "|        France|              110480.0|\n",
      "|     Australia|               83653.0|\n",
      "|        Sweden|               35637.0|\n",
      "|   Switzerland|               30325.0|\n",
      "|         Spain|               26824.0|\n",
      "|         Japan|               25218.0|\n",
      "+--------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "total_productos_por_pais_df = mi_df.groupBy(\"Country\").agg(sum(\"Quantity\").alias(\"TotalProductosVendidos\")).orderBy(\"TotalProductosVendidos\", ascending=False)\n",
    "total_productos_por_pais_df.show(10)\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT Country, SUM(Quantity) AS TotalProductosVendidos FROM retail GROUP BY Country ORDER BY TotalProductosVendidos DESC\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feafc7bc-922f-46a2-a1cb-7c5c78b1014f",
   "metadata": {},
   "source": [
    "9. Obtén la media de los precios unitarios de los productos vendidos agrupada por países"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1ede03e-6e12-4b14-804a-5deac392647b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|        Country|MediaPreciosUnitarios|\n",
      "+---------------+---------------------+\n",
      "|         Sweden|    3.910887445887446|\n",
      "|        Germany|    3.966929963138483|\n",
      "|         France|    5.028864087881269|\n",
      "|        Belgium|   3.6443354277428717|\n",
      "|        Finland|    5.448705035971222|\n",
      "|          Italy|    4.831120797011208|\n",
      "|           EIRE|    5.911077354807235|\n",
      "|      Lithuania|   2.8411428571428576|\n",
      "|         Norway|   6.0120257826887675|\n",
      "|          Spain|    4.987544413738654|\n",
      "|        Denmark|     3.25694087403599|\n",
      "|        Iceland|   2.6440109890109897|\n",
      "|         Israel|   3.6331313131313143|\n",
      "|Channel Islands|    4.932124010554089|\n",
      "|         Cyprus|    6.302363344051452|\n",
      "|    Switzerland|     3.40344155844156|\n",
      "|          Japan|   2.2761452513966476|\n",
      "|         Poland|    4.170879765395893|\n",
      "|       Portugal|     8.58297564186965|\n",
      "|      Australia|   3.2206115965051607|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+---------------------+\n",
      "|        Country|MediaPreciosUnitarios|\n",
      "+---------------+---------------------+\n",
      "|         Sweden|    3.910887445887446|\n",
      "|        Germany|    3.966929963138483|\n",
      "|         France|    5.028864087881269|\n",
      "|        Belgium|   3.6443354277428717|\n",
      "|        Finland|    5.448705035971222|\n",
      "|          Italy|    4.831120797011208|\n",
      "|           EIRE|    5.911077354807235|\n",
      "|      Lithuania|   2.8411428571428576|\n",
      "|         Norway|   6.0120257826887675|\n",
      "|          Spain|    4.987544413738654|\n",
      "|        Denmark|     3.25694087403599|\n",
      "|        Iceland|   2.6440109890109897|\n",
      "|         Israel|   3.6331313131313143|\n",
      "|Channel Islands|    4.932124010554089|\n",
      "|         Cyprus|    6.302363344051452|\n",
      "|    Switzerland|     3.40344155844156|\n",
      "|          Japan|   2.2761452513966476|\n",
      "|         Poland|    4.170879765395893|\n",
      "|       Portugal|     8.58297564186965|\n",
      "|      Australia|   3.2206115965051607|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "media_por_pais_df = mi_df.groupBy(\"Country\").agg(avg(\"UnitPrice\").alias(\"MediaPreciosUnitarios\"))\n",
    "media_por_pais_df.show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT Country, AVG(UnitPrice) AS MediaPreciosUnitarios FROM retail GROUP BY Country\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62faa914-1b71-4051-b2e3-e1e1454adada",
   "metadata": {},
   "source": [
    "10. Obtén el importe total (quantity * unit price) agrupado por número de factura (invoiceNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ef1acd-345e-4907-a5ab-107a408b999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|InvoiceNo|      ImporteTotal|\n",
      "+---------+------------------+\n",
      "|   536596|             38.09|\n",
      "|   536938|1680.8799999999999|\n",
      "|   537252|26.349999999999998|\n",
      "|   537691|            310.57|\n",
      "|   538041|               0.0|\n",
      "|   538184|458.91999999999985|\n",
      "|   538517|320.28000000000003|\n",
      "|   538879| 338.9799999999999|\n",
      "|   539275|403.79999999999995|\n",
      "|   539630|             751.0|\n",
      "|   540499|             365.2|\n",
      "|   536597|            102.79|\n",
      "|   536608|              38.1|\n",
      "|   536765|               0.0|\n",
      "|   537439|               0.0|\n",
      "|   537631|              17.4|\n",
      "|   537870|               0.0|\n",
      "|   538142|               0.0|\n",
      "|   538177| 6272.010000000006|\n",
      "|   538308|            363.24|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+------------------+\n",
      "|InvoiceNo|      ImporteTotal|\n",
      "+---------+------------------+\n",
      "|   536596|             38.09|\n",
      "|   536938|1680.8799999999999|\n",
      "|   537252|26.349999999999998|\n",
      "|   537691|            310.57|\n",
      "|   538041|               0.0|\n",
      "|   538184|458.91999999999985|\n",
      "|   538517|320.28000000000003|\n",
      "|   538879| 338.9799999999999|\n",
      "|   539275|403.79999999999995|\n",
      "|   539630|             751.0|\n",
      "|   540499|             365.2|\n",
      "|   536597|            102.79|\n",
      "|   536608|              38.1|\n",
      "|   536765|               0.0|\n",
      "|   537439|               0.0|\n",
      "|   537631|              17.4|\n",
      "|   537870|               0.0|\n",
      "|   538142|               0.0|\n",
      "|   538177| 6272.010000000006|\n",
      "|   538308|            363.24|\n",
      "+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "#DataFrame API\n",
    "total_por_factura_df = mi_df.withColumn(\"ImporteTotal\", expr(\"Quantity * UnitPrice\")).groupBy(\"InvoiceNo\").agg(sum(\"ImporteTotal\").alias(\"ImporteTotal\"))\n",
    "total_por_factura_df.show()\n",
    "\n",
    "#SQL\n",
    "spark.sql(\"SELECT InvoiceNo, SUM(Quantity * UnitPrice) AS ImporteTotal FROM retail GROUP BY InvoiceNo\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125443d3-7d2c-4cec-b97e-181a87ef4574",
   "metadata": {},
   "source": [
    "11. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. (sólo mostrar los alumnos que tengan nota en las 3 asignaturas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2110e4ec-d5e7-4289-98ad-414a8880b0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+----------+\n",
      "|   alumno|nota_fisica|nota_ingles|nota_mates|\n",
      "+---------+-----------+-----------+----------+\n",
      "|    Angel|          9|          4|       6.0|\n",
      "|    Maria|          3|          6|       2.0|\n",
      "|    Ramon|          7|          8|       4.5|\n",
      "|    Jorge|          5|          5|      10.0|\n",
      "|   Susana|          9|          2|       9.0|\n",
      "|   Anabel|          2|          7|       8.0|\n",
      "|    Rocio|          5|          4|       6.0|\n",
      "|   Carlos|          4|          8|       4.0|\n",
      "|    Rocio|          7|          4|       6.0|\n",
      "|   Triana|          3|          4|       3.0|\n",
      "|   Andres|          4|          6|       4.0|\n",
      "| Fernando|          9|          7|       5.0|\n",
      "| Leonardo|          6|          4|       1.0|\n",
      "|    Oscar|          5|          3|       7.0|\n",
      "|   Isabel|          8|          7|       8.0|\n",
      "|Jose Juan|          3|          3|       5.0|\n",
      "|  Nicolas|          7|          5|       2.0|\n",
      "|Alejandro|          3|          7|       5.0|\n",
      "|     Rosa|          8|          9|       6.0|\n",
      "+---------+-----------+-----------+----------+\n",
      "\n",
      "+---------+-----------+-----------+----------+\n",
      "|   alumno|nota_fisica|nota_ingles|nota_mates|\n",
      "+---------+-----------+-----------+----------+\n",
      "|    Angel|          9|          4|       6.0|\n",
      "|    Maria|          3|          6|       2.0|\n",
      "|    Ramon|          7|          8|       4.5|\n",
      "|    Jorge|          5|          5|      10.0|\n",
      "|   Susana|          9|          2|       9.0|\n",
      "|   Anabel|          2|          7|       8.0|\n",
      "|    Rocio|          5|          4|       6.0|\n",
      "|   Carlos|          4|          8|       4.0|\n",
      "|    Rocio|          7|          4|       6.0|\n",
      "|   Triana|          3|          4|       3.0|\n",
      "|   Andres|          4|          6|       4.0|\n",
      "| Fernando|          9|          7|       5.0|\n",
      "| Leonardo|          6|          4|       1.0|\n",
      "|    Oscar|          5|          3|       7.0|\n",
      "|   Isabel|          8|          7|       8.0|\n",
      "|Jose Juan|          3|          3|       5.0|\n",
      "|  Nicolas|          7|          5|       2.0|\n",
      "|Alejandro|          3|          7|       5.0|\n",
      "|     Rosa|          8|          9|       6.0|\n",
      "+---------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "notas_fisica_df = spark.read.csv(\"data/notas/notas_fisica.txt\", header=False, inferSchema=True).toDF(\"alumno\", \"nota_fisica\")\n",
    "notas_ingles_df = spark.read.csv(\"data/notas/notas_ingles.txt\", header=False, inferSchema=True).toDF(\"alumno\", \"nota_ingles\")\n",
    "notas_mates_df = spark.read.csv(\"data/notas/notas_mates.txt\", header=False, inferSchema=True).toDF(\"alumno\", \"nota_mates\")\n",
    "\n",
    "join_df = notas_fisica_df.join(notas_ingles_df, \"alumno\", \"inner\").join(notas_mates_df, \"alumno\", \"inner\")\n",
    "\n",
    "alumnos_notas_df = join_df.filter(\n",
    "    col(\"nota_fisica\").isNotNull() & col(\"nota_ingles\").isNotNull() & col(\"nota_mates\").isNotNull()\n",
    ")\n",
    "\n",
    "alumnos_notas_df.show()\n",
    "\n",
    "#SQL\n",
    "notas_fisica_df.createOrReplaceTempView(\"notas_fisica\")\n",
    "notas_ingles_df.createOrReplaceTempView(\"notas_ingles\")\n",
    "notas_mates_df.createOrReplaceTempView(\"notas_mates\")\n",
    "\n",
    "consulta_sql = \"\"\"\n",
    "    SELECT notas_fisica.alumno, nota_fisica, nota_ingles, nota_mates\n",
    "    FROM notas_fisica\n",
    "    JOIN notas_ingles ON notas_fisica.alumno = notas_ingles.alumno\n",
    "    JOIN notas_mates ON notas_fisica.alumno = notas_mates.alumno\n",
    "    WHERE nota_fisica IS NOT NULL AND nota_ingles IS NOT NULL AND nota_mates IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(consulta_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68b0a9-90b3-408a-be73-80ed93ab913d",
   "metadata": {},
   "source": [
    "12. Crea un DataFrame para cada archivo de notas (notas_fisica, notas_ingles y notas_matemáticas). Realiza un Join que genere un DataFrame mostrando las tres notas para cada alumno. Si a un alumno le falta alguna de las notas aparecerá el valor NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f88c5b68-5482-4035-a5a2-d08a30c11776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+----------+\n",
      "|   alumno|nota_fisica|nota_ingles|nota_mates|\n",
      "+---------+-----------+-----------+----------+\n",
      "|Alejandro|          3|          7|       5.0|\n",
      "|   Anabel|          2|          7|       8.0|\n",
      "|   Andres|          4|          6|       4.0|\n",
      "|    Angel|          9|          4|       6.0|\n",
      "|   Carlos|          4|          8|       4.0|\n",
      "| Fernando|          9|          7|       5.0|\n",
      "|   Isabel|          8|          7|       8.0|\n",
      "|    Jorge|          5|          5|      10.0|\n",
      "|Jose Juan|          3|          3|       5.0|\n",
      "| Leonardo|          6|          4|       1.0|\n",
      "|    Maria|          3|          6|       2.0|\n",
      "|  Nicolas|          7|          5|       2.0|\n",
      "|    Oscar|          5|          3|       7.0|\n",
      "|    Pedro|          2|       NULL|       5.0|\n",
      "|    Ramon|          7|          8|       4.5|\n",
      "|    Rocio|          5|          4|       6.0|\n",
      "|    Rocio|          7|          4|       6.0|\n",
      "|     Rosa|          8|          9|       6.0|\n",
      "|   Susana|          9|          2|       9.0|\n",
      "|   Triana|          3|          4|       3.0|\n",
      "+---------+-----------+-----------+----------+\n",
      "\n",
      "+---------+-----------+-----------+----------+\n",
      "|   alumno|nota_fisica|nota_ingles|nota_mates|\n",
      "+---------+-----------+-----------+----------+\n",
      "|Alejandro|          3|          7|       5.0|\n",
      "|   Anabel|          2|          7|       8.0|\n",
      "|   Andres|          4|          6|       4.0|\n",
      "|    Angel|          9|          4|       6.0|\n",
      "|   Carlos|          4|          8|       4.0|\n",
      "| Fernando|          9|          7|       5.0|\n",
      "|   Isabel|          8|          7|       8.0|\n",
      "|    Jorge|          5|          5|      10.0|\n",
      "|Jose Juan|          3|          3|       5.0|\n",
      "| Leonardo|          6|          4|       1.0|\n",
      "|    Maria|          3|          6|       2.0|\n",
      "|  Nicolas|          7|          5|       2.0|\n",
      "|    Oscar|          5|          3|       7.0|\n",
      "|    Pedro|          2|       NULL|       5.0|\n",
      "|    Ramon|          7|          8|       4.5|\n",
      "|    Rocio|          5|          4|       6.0|\n",
      "|    Rocio|          7|          4|       6.0|\n",
      "|     Rosa|          8|          9|       6.0|\n",
      "|   Susana|          9|          2|       9.0|\n",
      "|   Triana|          3|          4|       3.0|\n",
      "+---------+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DataFrame API\n",
    "join_df = notas_fisica_df.join(notas_ingles_df, [\"alumno\"], \"full_outer\").join(notas_mates_df, [\"alumno\"], \"full_outer\")\n",
    "join_df.show()\n",
    "\n",
    "#SQL\n",
    "consulta_sql = \"\"\"\n",
    "    SELECT notas_fisica.alumno, nota_fisica, nota_ingles, nota_mates\n",
    "    FROM notas_fisica\n",
    "    FULL OUTER JOIN notas_ingles ON notas_fisica.alumno = notas_ingles.alumno\n",
    "    FULL OUTER JOIN notas_mates ON notas_fisica.alumno = notas_mates.alumno\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(consulta_sql).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
