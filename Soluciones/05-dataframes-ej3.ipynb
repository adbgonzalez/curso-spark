{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3fcfaa-acfd-4e41-8c68-13eca1782172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"05-Dataframes-ej03\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"hdfs:///spark/logs/history\") \\\n",
    "    .config(\"spark.history.fs.logDirectory\", \"hdfs:///spark/logs/history\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04733e26-bd1c-45d4-83c5-d19664cb9ff7",
   "metadata": {},
   "source": [
    "1. A partir de los ficheros csv presentes en /user/jovyan/data/salesdata crea un dataframe y añádele una columna date a partir de la columna `Order Date` que tenga fecha pero no hora. El dataframe resultado se llamará df_con_fecha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e73fc02-d0e6-43f9-b73a-87ceef879d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Purchase Address|      date|\n",
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|136 Church St, Ne...|0019-12-30|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|562 2nd St, New Y...|0019-12-29|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|277 Main St, New ...|0019-12-12|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|410 6th St, San F...|0019-12-22|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|43 Hill St, Atlan...|0019-12-18|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|200 Jefferson St,...|0019-12-31|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|928 12th St, Port...|0019-12-16|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|813 Hickory St, D...|0019-12-13|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|718 Wilson St, Da...|0019-12-15|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|77 7th St, Dallas...|0019-12-28|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|594 1st St, San F...|0019-12-13|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|410 Lincoln St, L...|0019-12-28|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|866 Pine St, Bost...|0019-12-20|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|187 Lincoln St, D...|0019-12-06|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|902 2nd St, Dalla...|0019-12-25|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|338 Main St, Aust...|0019-12-01|\n",
      "|  295681|        Google Phone|               1|     600.0|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|    Wired Headphones|               1|     11.99|79 Elm St, Boston...|0019-12-25|\n",
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import to_timestamp, date_format, col\n",
    "schema = StructType([\n",
    "    StructField(\"Order ID\", StringType(), True),          # Pode haber valores nulos\n",
    "    StructField(\"Product\", StringType(), True),\n",
    "    StructField(\"Quantity Ordered\", IntegerType(), True), # Ten que converterse a número\n",
    "    StructField(\"Price Each\", DoubleType(), True),        # Prezo decimal\n",
    "    StructField(\"Order Date\", StringType(), True),        # Podemos converter a Timestamp máis tarde\n",
    "    StructField(\"Purchase Address\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"hdfs:/user/jovyan/data/salesdata\")\n",
    "df_con_fecha = df.withColumn(\"date\", date_format(to_timestamp(col(\"Order Date\"), \"MM/d/yyyy H:mm\"), \"yyyy-MM-dd\")).drop(\"Order Date\")\n",
    "df_con_fecha.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32478d7e-7ac0-45b3-87e0-7eaa342c6bf0",
   "metadata": {},
   "source": [
    "2. Crea una especificación de ventana (*windowSpec*) particionando por *Order ID* y *date*, ordenando descendentemente por *Quantity Ordered*. Se debe incluir en el marco todas las entradas anteriores a la actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3994f76-dad0-4345-9b38-45a16c3e5ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window\\\n",
    "    .partitionBy(\"Order ID\", \"date\")\\\n",
    "    .orderBy(desc(\"Quantity Ordered\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3b700-1036-4714-8890-5e4de45a6086",
   "metadata": {},
   "source": [
    "3. Crea una agregación que calcule la cantidad (*Quantity Ordered*) media para todos los tiempos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac276df1-6414-4c86-af76-31cc6ba59511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "avgQuantity =avg(col(\"Quantity Ordered\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823fa436-34d5-478e-81cb-21f048386c54",
   "metadata": {},
   "source": [
    "4. Crea dos *rankings*, uno denso y el otro no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acd14147-7571-453b-8ef8-c189eb487306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|    Purchase Address|      date|\n",
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "|  295665|  Macbook Pro Laptop|               1|    1700.0|136 Church St, Ne...|0019-12-30|\n",
      "|  295666|  LG Washing Machine|               1|     600.0|562 2nd St, New Y...|0019-12-29|\n",
      "|  295667|USB-C Charging Cable|               1|     11.95|277 Main St, New ...|0019-12-12|\n",
      "|  295668|    27in FHD Monitor|               1|    149.99|410 6th St, San F...|0019-12-22|\n",
      "|  295669|USB-C Charging Cable|               1|     11.95|43 Hill St, Atlan...|0019-12-18|\n",
      "|  295670|AA Batteries (4-p...|               1|      3.84|200 Jefferson St,...|0019-12-31|\n",
      "|  295671|USB-C Charging Cable|               1|     11.95|928 12th St, Port...|0019-12-16|\n",
      "|  295672|USB-C Charging Cable|               2|     11.95|813 Hickory St, D...|0019-12-13|\n",
      "|  295673|Bose SoundSport H...|               1|     99.99|718 Wilson St, Da...|0019-12-15|\n",
      "|  295674|AAA Batteries (4-...|               4|      2.99|77 7th St, Dallas...|0019-12-28|\n",
      "|  295675|USB-C Charging Cable|               2|     11.95|594 1st St, San F...|0019-12-13|\n",
      "|  295676|     ThinkPad Laptop|               1|    999.99|410 Lincoln St, L...|0019-12-28|\n",
      "|  295677|AA Batteries (4-p...|               2|      3.84|866 Pine St, Bost...|0019-12-20|\n",
      "|  295678|AAA Batteries (4-...|               2|      2.99|187 Lincoln St, D...|0019-12-06|\n",
      "|  295679|USB-C Charging Cable|               1|     11.95|902 2nd St, Dalla...|0019-12-25|\n",
      "|  295680|Lightning Chargin...|               1|     14.95|338 Main St, Aust...|0019-12-01|\n",
      "|  295681|        Google Phone|               1|     600.0|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|USB-C Charging Cable|               1|     11.95|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|Bose SoundSport H...|               1|     99.99|79 Elm St, Boston...|0019-12-25|\n",
      "|  295681|    Wired Headphones|               1|     11.99|79 Elm St, Boston...|0019-12-25|\n",
      "+--------+--------------------+----------------+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank\n",
    "denseRank = dense_rank().over(windowSpec)\n",
    "rank = rank().over(windowSpec)\n",
    "\n",
    "df_con_fecha.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab62e7ed-9fd4-4197-8f60-32ab86c32b6d",
   "metadata": {},
   "source": [
    "5. Muestra las filas donde *Order ID* no es NULL, ordenadas por *Order ID*. Se deben mostrar los campos *Order ID*, *Product* y *date* así como los dos *rankings* y la función de agregación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b2719c-dc17-4550-bcbd-bcfffb017a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+----+---------+-----------+\n",
      "|Order ID|             Product|      date|rank|denseRank|avgQuantity|\n",
      "+--------+--------------------+----------+----+---------+-----------+\n",
      "|  141234|              iPhone|0019-01-22|   1|        1|        1.0|\n",
      "|  141235|Lightning Chargin...|0019-01-28|   1|        1|        1.0|\n",
      "|  141236|    Wired Headphones|0019-01-17|   1|        1|        2.0|\n",
      "|  141237|    27in FHD Monitor|0019-01-05|   1|        1|        1.0|\n",
      "|  141238|    Wired Headphones|0019-01-25|   1|        1|        1.0|\n",
      "|  141239|AAA Batteries (4-...|0019-01-29|   1|        1|        1.0|\n",
      "|  141240|27in 4K Gaming Mo...|0019-01-26|   1|        1|        1.0|\n",
      "|  141241|USB-C Charging Cable|0019-01-05|   1|        1|        1.0|\n",
      "|  141242|Bose SoundSport H...|0019-01-01|   1|        1|        1.0|\n",
      "|  141243|Apple Airpods Hea...|0019-01-22|   1|        1|        1.0|\n",
      "|  141244|Apple Airpods Hea...|0019-01-07|   1|        1|        1.0|\n",
      "|  141245|  Macbook Pro Laptop|0019-01-31|   1|        1|        1.0|\n",
      "|  141246|AAA Batteries (4-...|0019-01-09|   1|        1|        3.0|\n",
      "|  141247|    27in FHD Monitor|0019-01-25|   1|        1|        1.0|\n",
      "|  141248|       Flatscreen TV|0019-01-03|   1|        1|        1.0|\n",
      "|  141249|    27in FHD Monitor|0019-01-05|   1|        1|        1.0|\n",
      "|  141250|     Vareebadd Phone|0019-01-10|   1|        1|        1.0|\n",
      "|  141251|Apple Airpods Hea...|0019-01-24|   1|        1|        1.0|\n",
      "|  141252|USB-C Charging Cable|0019-01-30|   1|        1|        1.0|\n",
      "|  141253|AA Batteries (4-p...|0019-01-17|   1|        1|        1.0|\n",
      "+--------+--------------------+----------+----+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_con_fecha.where(col(\"Order ID\").isNotNull()).orderBy(\"Order ID\")\\\n",
    "  .select(\n",
    "    col(\"Order ID\"),\n",
    "    col(\"Product\"),\n",
    "    col(\"date\"),\n",
    "    rank.alias(\"rank\"),\n",
    "    denseRank.alias(\"denseRank\"),\n",
    "    avgQuantity.alias(\"avgQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d241264-3492-4711-8a2c-ad463c17ca1e",
   "metadata": {},
   "source": [
    "6. Importa el archivo *hdfs:///user/jovyan/data/bike-data/201508_station_data.csv* incluyendo cabeceras e infiriendo el esquema a un *dataframe* llamado *df_estaciones*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2acaab8a-c07e-4ecf-9e88-32aec26ff7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|station_id|                name|      lat|       long|dockcount|    landmark|installation|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "|         2|San Jose Diridon ...|37.329732|-121.901782|       27|    San Jose|    8/6/2013|\n",
      "|         3|San Jose Civic Ce...|37.330698|-121.888979|       15|    San Jose|    8/5/2013|\n",
      "|         4|Santa Clara at Al...|37.333988|-121.894902|       11|    San Jose|    8/6/2013|\n",
      "|         5|    Adobe on Almaden|37.331415|  -121.8932|       19|    San Jose|    8/5/2013|\n",
      "|         6|    San Pedro Square|37.336721|-121.894074|       15|    San Jose|    8/7/2013|\n",
      "|         7|Paseo de San Antonio|37.333798|-121.886943|       15|    San Jose|    8/7/2013|\n",
      "|         8| San Salvador at 1st|37.330165|-121.885831|       15|    San Jose|    8/5/2013|\n",
      "|         9|           Japantown|37.348742|-121.894715|       15|    San Jose|    8/5/2013|\n",
      "|        10|  San Jose City Hall|37.337391|-121.886995|       15|    San Jose|    8/6/2013|\n",
      "|        11|         MLK Library|37.335885| -121.88566|       19|    San Jose|    8/6/2013|\n",
      "|        12|SJSU 4th at San C...|37.332808|-121.883891|       19|    San Jose|    8/7/2013|\n",
      "|        13|       St James Park|37.339301|-121.889937|       15|    San Jose|    8/6/2013|\n",
      "|        14|Arena Green / SAP...|37.332692|-121.900084|       19|    San Jose|    8/5/2013|\n",
      "|        16|SJSU - San Salvad...|37.333955|-121.877349|       15|    San Jose|    8/7/2013|\n",
      "|        21|   Franklin at Maple|37.481758|-122.226904|       15|Redwood City|   8/12/2013|\n",
      "|        22|Redwood City Calt...|37.486078|-122.232089|       25|Redwood City|   8/15/2013|\n",
      "|        23|San Mateo County ...|37.487616|-122.229951|       15|Redwood City|   8/15/2013|\n",
      "|        24|Redwood City Publ...|37.484219|-122.227424|       15|Redwood City|   8/12/2013|\n",
      "|        25|Stanford in Redwo...| 37.48537|-122.203288|       15|Redwood City|   8/12/2013|\n",
      "|        26|Redwood City Medi...|37.487682|-122.223492|       15|Redwood City|   8/12/2013|\n",
      "+----------+--------------------+---------+-----------+---------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estaciones = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"hdfs:///user/jovyan/data/bike-data/201508_station_data.csv\").drop()\n",
    "\n",
    "df_estaciones.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b22ec-691a-4fad-a5e9-109e7d3480f6",
   "metadata": {},
   "source": [
    "7. Crea una agregación que sume la columna *dockcount* agregando por *landmark* y *station_id* empleando *GROUPING SETS*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c690ba0d-4bed-4ec7-8317-2bf68f27a768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----------+\n",
      "|     landmark|station_id|total_docks|\n",
      "+-------------+----------+-----------+\n",
      "|Mountain View|        27|         15|\n",
      "|Mountain View|        28|         23|\n",
      "|Mountain View|        29|         23|\n",
      "|Mountain View|        30|         15|\n",
      "|Mountain View|        31|         15|\n",
      "|Mountain View|        32|         11|\n",
      "|Mountain View|        33|         15|\n",
      "|    Palo Alto|        34|         23|\n",
      "|    Palo Alto|        35|         11|\n",
      "|    Palo Alto|        36|         15|\n",
      "|    Palo Alto|        37|         11|\n",
      "|    Palo Alto|        38|         15|\n",
      "| Redwood City|        21|         15|\n",
      "| Redwood City|        22|         25|\n",
      "| Redwood City|        23|         15|\n",
      "| Redwood City|        24|         15|\n",
      "| Redwood City|        25|         15|\n",
      "| Redwood City|        26|         15|\n",
      "| Redwood City|        83|         15|\n",
      "|San Francisco|        39|         19|\n",
      "+-------------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_estaciones.createOrReplaceTempView(\"estaciones\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        landmark, \n",
    "        station_id, \n",
    "        SUM(dockcount) AS total_docks\n",
    "    FROM estaciones\n",
    "    GROUP BY GROUPING SETS (\n",
    "        (landmark, \n",
    "        station_id)\n",
    "    )\n",
    "    ORDER BY landmark, station_id\n",
    "\"\"\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269c0a4-db64-415a-9eb9-ff3c8f4503fe",
   "metadata": {},
   "source": [
    "8. Crea una agregación que sume la columna *dockcount* agregando por *landmark* y *station_id* empleando *GROUPING SETS* que también muestre los totales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41b88be0-4b8f-4aad-8541-c92c26b67a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|landmark|station_id|total_docks|\n",
      "+--------+----------+-----------+\n",
      "|NULL    |2         |27         |\n",
      "|NULL    |3         |15         |\n",
      "|NULL    |4         |11         |\n",
      "|NULL    |5         |19         |\n",
      "|NULL    |6         |15         |\n",
      "|NULL    |7         |15         |\n",
      "|NULL    |8         |15         |\n",
      "|NULL    |9         |15         |\n",
      "|NULL    |10        |15         |\n",
      "|NULL    |11        |19         |\n",
      "|NULL    |12        |19         |\n",
      "|NULL    |13        |15         |\n",
      "|NULL    |14        |19         |\n",
      "|NULL    |16        |15         |\n",
      "|NULL    |21        |15         |\n",
      "|NULL    |22        |25         |\n",
      "|NULL    |23        |15         |\n",
      "|NULL    |24        |15         |\n",
      "|NULL    |25        |15         |\n",
      "|NULL    |26        |15         |\n",
      "+--------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        landmark, \n",
    "        station_id, \n",
    "        SUM(dockcount) AS total_docks\n",
    "    FROM estaciones\n",
    "    GROUP BY GROUPING SETS (\n",
    "        (landmark), \n",
    "        (station_id), \n",
    "        (landmark, station_id)\n",
    "    )\n",
    "    ORDER BY landmark, station_id\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f9f9c-bac5-4cbb-b4a3-5f297a0188c6",
   "metadata": {},
   "source": [
    "9. Haz un rollup de *df_estaciones* con los parámetros *landmark* y *station_id*. Agrega la suma de *dockcount* y selecciona esas tres columnas, almacenando el resultado como *df_enroscado*. Muestra la siguiente información:\n",
    "  -  a. Todas las filas.\n",
    "  -  b. Todas las filas en las que *landmark* es NULL.\n",
    "  -  c. Todas las filas en las que *station_id* es NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "275c20ee-87ad-48af-95ac-ba312ce8033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dockcount: integer (nullable = true)\n",
      " |-- landmark: string (nullable = true)\n",
      " |-- installation: string (nullable = true)\n",
      "\n",
      "a. Todas las filas: \n",
      "+-------------+----------+---------------+\n",
      "|     landmark|station_id|total_dockcount|\n",
      "+-------------+----------+---------------+\n",
      "|         NULL|      NULL|           1236|\n",
      "|Mountain View|        31|             15|\n",
      "|Mountain View|        30|             15|\n",
      "|Mountain View|        33|             15|\n",
      "|Mountain View|        29|             23|\n",
      "|Mountain View|      NULL|            117|\n",
      "|Mountain View|        28|             23|\n",
      "|Mountain View|        32|             11|\n",
      "|Mountain View|        27|             15|\n",
      "|    Palo Alto|        38|             15|\n",
      "|    Palo Alto|      NULL|             75|\n",
      "|    Palo Alto|        36|             15|\n",
      "|    Palo Alto|        35|             11|\n",
      "|    Palo Alto|        34|             23|\n",
      "|    Palo Alto|        37|             11|\n",
      "| Redwood City|        83|             15|\n",
      "| Redwood City|        26|             15|\n",
      "| Redwood City|        23|             15|\n",
      "| Redwood City|        24|             15|\n",
      "| Redwood City|        25|             15|\n",
      "+-------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "b.Todas las filas en las que landmark es nULL: \n",
      "+--------+----------+---------------+\n",
      "|landmark|station_id|total_dockcount|\n",
      "+--------+----------+---------------+\n",
      "|    NULL|      NULL|           1236|\n",
      "+--------+----------+---------------+\n",
      "\n",
      "c.Todas las filas en las que station_id es nULL: \n",
      "+-------------+----------+---------------+\n",
      "|     landmark|station_id|total_dockcount|\n",
      "+-------------+----------+---------------+\n",
      "|         NULL|      NULL|           1236|\n",
      "|Mountain View|      NULL|            117|\n",
      "|    Palo Alto|      NULL|             75|\n",
      "| Redwood City|      NULL|            115|\n",
      "|San Francisco|      NULL|            665|\n",
      "|     San Jose|      NULL|            264|\n",
      "+-------------+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df_estaciones.printSchema()\n",
    "df_enroscado = df_estaciones.rollup(\"landmark\", \"station_id\").agg(sum(\"dockcount\")).selectExpr(\"landmark\", \"station_id\", \"`sum(dockcount)` as total_dockcount\").orderBy(\"landmark\")\n",
    "print(\"a. Todas las filas: \")\n",
    "df_enroscado.show()\n",
    "\n",
    "print (\"b.Todas las filas en las que landmark es nULL: \")\n",
    "df_enroscado.filter(\"landmark is NULL\").show()\n",
    "\n",
    "print (\"c.Todas las filas en las que station_id es nULL: \")\n",
    "df_enroscado.filter(\"station_id is NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cbafc-dd9c-4cd1-a902-c2e130dc67e4",
   "metadata": {},
   "source": [
    " 10. Haz un cubo de *df_estaciones* con los parámetros *landmark* y *station_id*. Agrega la suma de *dockcount* y selecciona esas tres columnas, almacenando el resultado como *df_cubo*. Muestra la siguiente información:\n",
    "  -  a. Todas las filas.\n",
    "  -  b. Todas las filas en las que *landmark* es NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95050709-ad65-46f7-8348-986f032597f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a. Todas las filas: \n",
      "+--------+----------+---------------+\n",
      "|landmark|station_id|total_dockcount|\n",
      "+--------+----------+---------------+\n",
      "|    NULL|        74|             23|\n",
      "|    NULL|         4|             11|\n",
      "|    NULL|         7|             15|\n",
      "|    NULL|        69|             23|\n",
      "|    NULL|         6|             15|\n",
      "|    NULL|        12|             19|\n",
      "|    NULL|        48|             15|\n",
      "|    NULL|        42|             15|\n",
      "|    NULL|        29|             23|\n",
      "|    NULL|        25|             15|\n",
      "|    NULL|        26|             15|\n",
      "|    NULL|        11|             19|\n",
      "|    NULL|        37|             11|\n",
      "|    NULL|        54|             15|\n",
      "|    NULL|        21|             15|\n",
      "|    NULL|        62|             19|\n",
      "|    NULL|        55|             23|\n",
      "|    NULL|        58|             19|\n",
      "|    NULL|        57|             15|\n",
      "|    NULL|        84|             15|\n",
      "+--------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "b. Todas las filas en las que landmark es NULL\n",
      "+--------+----------+---------------+\n",
      "|landmark|station_id|total_dockcount|\n",
      "+--------+----------+---------------+\n",
      "|    NULL|        58|             19|\n",
      "|    NULL|        12|             19|\n",
      "|    NULL|        42|             15|\n",
      "|    NULL|        48|             15|\n",
      "|    NULL|        21|             15|\n",
      "|    NULL|        29|             23|\n",
      "|    NULL|        62|             19|\n",
      "|    NULL|        26|             15|\n",
      "|    NULL|        55|             23|\n",
      "|    NULL|        37|             11|\n",
      "|    NULL|        74|             23|\n",
      "|    NULL|         6|             15|\n",
      "|    NULL|         7|             15|\n",
      "|    NULL|        25|             15|\n",
      "|    NULL|        11|             19|\n",
      "|    NULL|        57|             15|\n",
      "|    NULL|        84|             15|\n",
      "|    NULL|        16|             15|\n",
      "|    NULL|        54|             15|\n",
      "|    NULL|        69|             23|\n",
      "+--------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cubo = df_estaciones.cube(\"landmark\", \"station_id\").agg(sum(\"dockcount\").alias(\"total_dockcount\")).selectExpr(\"landmark\", \"station_id\", \"total_dockcount\").orderBy(\"landmark\")\n",
    "print (\"a. Todas las filas: \")\n",
    "df_cubo.show()\n",
    "\n",
    "print (\"b. Todas las filas en las que landmark es NULL\")\n",
    "df_cubo.filter(\"landmark is NULL\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
