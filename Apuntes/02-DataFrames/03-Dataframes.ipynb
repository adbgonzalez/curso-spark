{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4079f44-0788-4243-af2c-fc7ea9dea98c",
   "metadata": {},
   "source": [
    "# Dataframes\n",
    "Los **DataFrames** son una parte fundamental de la **API Estructurada de Spark** (*Spark's Structured API*). Esta API permite manipular ficheros desustructurados, semi estructurados (ejemplo: CSV) o ficheros altamente estructurados (Parquet). Se contemplan tres tipos de colecciones distribuídas:\n",
    "- **Datasets**: Versión más fuertemente tipada de los *DataFrames*. Disponible solamante en lenguajes basados en JVM, como Java o Scala.\n",
    "- **DataFrames**: Colecciones con forma de tabla, con filas y columnas bien definidas. A diferencia de los *DataSets* el tipado no es tan fuerte y solamente se comprueba en tiempo de ejecución.\n",
    "- **Tablas y vistas de SQL**: Se pueden crear tablas/vistas a partir de *DataFrames*. La función *spark.sql()* permite realizar consultas SQL directamente sobre ellas.\n",
    "\n",
    "El proceso que se sigue para ejecutar un trabajo con la API Estructurada de Spark es el siguiente:\n",
    "1. Escribir el código para manejar DataFrames/DataSets/SQL.\n",
    "2. Si el código es válido *Spark* lo convierte en un *Plan Lógico*\n",
    "3. Spark convierte el *Plan Lógico* en un *Plan Físico*, aplicando optimizaciones en el proceso.\n",
    "4. Spark ejecuta el *Plan Físico*, consistente en operaciones con *RDD's* a bajo nivel a través del clúster.\n",
    "\n",
    "El *Plan Lógico* representa una serie de transformaciones abstractas sin hacer referencia a *ejecutores* o *drivers*. A partir del *código del usuario* se genera un *Plan lógico sin resolver* que es analizado con la ayuda del *Catálogo*, un repositorio que comprueba que las tablas, columnas, etc. existen de verdad y son correctas dando lugar a un *plan lógico resuelto*. Por último se aplica un proceso de optimización para obtener el *Plan lógico optimizado*.\n",
    "![Plan lógico](./images/logicalplan.png)\n",
    "El *Plan físico* (*Plan Spark*) especifica como se ejecutará el plan lógico en el clúster, generando diferentes estrategias y comparándolas en base a un modelo de coste.\n",
    "![Plan físico](./images/physicalplan.png)\n",
    "El resultado es una serie de *RDD's* y transformaciones. En definitiva, se transforman consultas sobre DataFrames/Datasets o SQL en transformaciones de *RDD's*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3d662-7f60-4462-a752-4f86428df707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inicializamos SparkSession y SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"03-Dataframes\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    .config(\"spark.eventLog.dir\", \"hdfs:///spark/logs/history\") \\\n",
    "    .config(\"spark.history.fs.logDirectory\", \"hdfs:///spark/logs/history\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version  # Verifica la versión de Spark\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bc0cb-2041-4de4-9b39-3b8327ffbe4a",
   "metadata": {},
   "source": [
    "## Creación de Dataframes\n",
    "Se pueden crear Dataframes a partir de diversas fuentes:\n",
    "### A partir de un RDD\n",
    "toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae0545-2f1e-41cf-a251-2cfdb860c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df =spark.range(50).toDF(\"number\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da85ee2a-b7d4-4f04-8996-e8993a3a6577",
   "metadata": {},
   "source": [
    "### A partir de un archivo\n",
    "- json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8350a3a2-298b-4232-9db0-d573131142db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.json(\"data/flight-data/json/2010-summary.json\")\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece556d5-e35e-4025-b59c-66e14a57d825",
   "metadata": {},
   "source": [
    "- csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf968800-72bd-46d1-b85e-4cec13b7594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.option(\"header\", True).csv('/home/jovyan/work/data/flight-data/csv/2015-summary.csv').coalesce(5)\n",
    "df_csv.show(10)\n",
    "df_csv.printSchema()\n",
    "df_csv.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304fc21-2ae7-4491-a6b5-9e4dc2cce4a8",
   "metadata": {},
   "source": [
    "- parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c6885-2349-4f67-8326-759b239c9f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(\"data/flight-data/parquet/2010-summary.parquet\")\n",
    "df_parquet.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4fe5e-64d7-44a7-86a7-24d7614fffaf",
   "metadata": {},
   "source": [
    "### Esquemas\n",
    "Aunque los *DataFrames no sean tan fuertemente tipados como los *Datasets* sí que se le puede asignar un tipo a cada campo, aunque la comprobación solamente se haga en tiempo de ejecución. Para conocer el esquema de un *DataFrame* podemos usar el método *printSchema*. A este respecto tenemos dos opciones:\n",
    "- Especificar el esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc57f2-5cf1-4d2d-ab13-418772ae9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Definir el esquema\n",
    "\n",
    "schema = StructType([\n",
    "\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Leer el archivo CSV con el esquema especificado\n",
    "\n",
    "df = spark.read.csv(\"/home/jovyan/work/data/retail-data/all/online-retail-dataset.csv\", header=True, schema=schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b104d1c-5c97-4239-a444-2e0ad8698394",
   "metadata": {},
   "source": [
    "- Inferir el esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39d8be-da09-4cd0-844f-a4b6d965e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.option(\"header\", True).option(\"inferSchema\",\"true\").csv('/home/jovyan/work/data/flight-data/csv/2015-summary.csv').coalesce(5)\n",
    "df_csv.show(10)\n",
    "df_csv.printSchema()\n",
    "df_csv.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e7c0a1-0546-48c0-8c15-87f165df2256",
   "metadata": {},
   "source": [
    "## Persistencia de datos\n",
    "A su vez, Spark permite almacenar los datos en diversos tipos de archivos usando los diferentes métodos de *DataFrame.write* (para json *write.json*). Algunas de las opciones son las siguientes:\n",
    "- *header*: Si queremos almacenar o no la cabecera.\n",
    "- *mode*: Si queremos sobreescribir o no (para sobreescribir mode=*overwrite*).\n",
    "\n",
    "NOTA: Debido a problemas de permisos con los volúmenes de Docker, en estos cuadernos deberemos hacerlo fuera del directorio *work*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cd9ae-3735-40c0-b30a-58ef6f954fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df_csv.write.csv(\"/home/jovyan/salida/datos.csv\", header= True, mode=\"overwrite\")\n",
    "# df_csv.write.json(\"/home/jovyan/salida/datos.json\", mode=\"overwrite\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b655c0-4f20-4e4f-8791-fe38521e5c34",
   "metadata": {},
   "source": [
    "## Transformacioness\n",
    "A continuación vamos a ver algunas de las principales transformaciones permitidas para Dataframes\n",
    "### select\n",
    "Permiten hacer una operación equivalente al select de las BBDDR. Dos variantes:\n",
    "- **select**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9ef5d-9a02-4034-b843-8a9d07ca8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.select(\"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3ae7f-f10c-47bc-84b6-d177daa761db",
   "metadata": {},
   "source": [
    "Podemos usar lal función **expr** para diversas acciones, por ejemplo, usar alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ce9cf-9c23-479d-9557-b4d3f20b1ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "df_csv.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d4452-2ecd-4dc4-8e56-9234212863a1",
   "metadata": {},
   "source": [
    "- **selectExpr**:\n",
    "Permite añadir expresiones de una forma más sencilla. Podemos hacer operaciones sencillas como usar alias (1) u operaciones más complejas como añadir columnas (2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee9db7-da3f-4722-92ff-6514fb365536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:\n",
    "df_csv.selectExpr(\"DEST_COUNTRY_NAME AS Destino\", \"ORIGIN_COUNTRY_NAME AS Origen\").show(5)\n",
    "# 2:\n",
    "df_csv.selectExpr(\"*\", \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) AS withinCountry\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948f67f-a1a6-4d80-b70c-2633b68dc1fd",
   "metadata": {},
   "source": [
    "### Añadir columnas\n",
    "Sin embargo, para añadir columnas puede ser más sencillo el método **withColumn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29168119-5372-4d9c-a418-8e35d99124b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba7673-7530-48b4-821f-b250d7b13b11",
   "metadata": {},
   "source": [
    "### Renombrar columnas\n",
    "De la misma forma, **withColumnRenamed** permite usar alias de forma más directa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742cbf5f-a238-490b-a292-f86f31197430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"dest\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea7fb4-8bee-438b-810e-be151228151d",
   "metadata": {},
   "source": [
    "### Eliminar columnas\n",
    "La función **drop** permite eliminar una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103259d3-b335-4a24-890d-1363d0fee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.drop(\"ORIGIN_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f137720-8b7e-4431-b41a-db67c7981ba9",
   "metadata": {},
   "source": [
    "### Filtrar filas\n",
    "Las funciones **where/filter** permiten filtrar el DF por un criterio, de forma similar a la cláusula Where de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bd2fc-3bb8-467e-8144-1852a9b51690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.filter(\"DEST_COUNTRY_NAME = 'United States' AND count < 5\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a03f7-be1d-4f5d-9d30-c26f58b748b2",
   "metadata": {},
   "source": [
    "### Obtener filas únicas\n",
    "La función **distinct** permite elminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba2c1a4-2bd9-4c65-af75-eeef9403a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf287a1-28fe-4966-871a-20b617260db5",
   "metadata": {},
   "source": [
    "### Ordenando filas\n",
    "Con **sort** podemos ordenar el DF en función de la columna especificada. Las funciones **asc** y **desc** nos permiten especificar el sentido de la ordenación (por defecto ascendente)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad527a-25f7-4712-b510-c56e7f6e9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b6b23-5ba4-4ddd-8c6d-832782108fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "df_csv.sort(col('count').desc(), col('ORIGIN_COUNTRY_NAME').asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea251727-0550-4cb6-b33d-4a02ffa8a5e2",
   "metadata": {},
   "source": [
    "### limit\n",
    "Permite limitar el número de filas obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a5f77-718a-43dc-8ecc-430315279853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cee1d5-a219-415a-b694-f58c006eac67",
   "metadata": {},
   "source": [
    "## Trabajando con particiones\n",
    "Utilizando el atributo *rdd* de la clase *Dataframe*se pueden utilizar las funciones vistas para la API de RDD's.\n",
    "### getNumPartitions\n",
    "Permite obtener el número de particiones de un *DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b1a99-86f7-400e-ac09-443603768947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rdd.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555dda32-ec51-477d-b123-4728ca783346",
   "metadata": {},
   "source": [
    "### repartition\n",
    "Permite cambiar el número de particiones, después de *barajar* los datos. Permite varias opciones:\n",
    "- Indicando sólo el número de particiones deseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae24358-9fc2-4776-b9e9-3572d33175e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rdd.repartition(5)\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48caf381-e21c-4d64-ad24-4cd625f5824d",
   "metadata": {},
   "source": [
    "- Indicando la columna por la cual queremos \"reparticionar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365198f-1422-4cb1-a6c6-d487227449ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2.repartition(col(\"DEST_COUNTRY_NAME\"))\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e5284-7e53-4c6f-84a7-63323ffc8ad8",
   "metadata": {},
   "source": [
    "- Indicando ambas cosas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5d7e5-d8ea-4691-b755-559e222be532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2.repartition (5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc9f893-84ba-4039-97ce-2f00c98308e2",
   "metadata": {},
   "source": [
    "## Agregaciones\n",
    "Spark permite el uso de diversas funciones de agregación con *Dataframes*. Para ello vamos a usar el siguiente DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b257295-194b-4347-9076-2defa2c6c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ag = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"/home/jovyan/work/data/retail-data/all/*.csv\")\\\n",
    "  .coalesce(5)\n",
    "df_ag.cache()\n",
    "df_ag.createOrReplaceTempView(\"dfTable\")\n",
    "df_ag.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff1584-8f39-4514-afab-2dd3b5b8f16b",
   "metadata": {},
   "source": [
    "A continuación, algunas de las más importantes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e178d-3c39-4d88-b9c6-f506108c2185",
   "metadata": {},
   "source": [
    "### count \n",
    "Podemos usarla de tres formas:\n",
    "- Especificanco una columna\n",
    "- Especificando todas las columnas (*)\n",
    "- Contar todas las filas como un literal (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a363aa0-b2ef-42eb-8dc7-acdfbb8a2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df_ag.select(count(\"InvoiceNo\")).show()\n",
    "\n",
    "df_ag.select(count(\"*\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7753ca-1755-4ce9-b559-5fc29671cbf0",
   "metadata": {},
   "source": [
    "### countDistinct: \n",
    "Cuenta el número de ocurrencias distintas de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4998b-74e9-495a-8565-e10f89a6cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df_ag.select(countDistinct(\"InvoiceNo\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05c09-ceb4-458a-b1d3-7675a5dd10f7",
   "metadata": {},
   "source": [
    "### approx_count_distinct\n",
    "Al trabajar con grandes conjuntos de datos puede interesar una aproximación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec96f67-3c14-436e-9cc2-0b300c6e1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df_ag.select(approx_count_distinct(\"InvoiceNo\",0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85951668-7fe7-4fe2-b43b-0749aae9dd63",
   "metadata": {},
   "source": [
    "### first/last\n",
    "Permiten obtener el *primero* o el *último* elemento de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ac3d1-3568-4c50-b5de-ed8739810bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "df_ag.orderBy(\"Country\").select(first(\"Country\"), last(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269b012-23a9-445b-afda-743886804dfe",
   "metadata": {},
   "source": [
    "### min/max\n",
    "Permiten obtener el valor *máximo* o *mínimo* de una columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb3565-19a1-47b4-a2c6-66aee0ba3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max,min\n",
    "df_ag.select(max(\"Quantity\"),min(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd387bd-4911-477f-9394-4f0647ac8be0",
   "metadata": {},
   "source": [
    "### sum, sumDistinct\n",
    "Mientras que **sum** realiza la suma de todos los elementos de una columna **sum_distinct** hace lo mismo pero eliminando antes los repetidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55209eef-0dd1-476c-b1f2-108eab5b2fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, sum_distinct\n",
    "df_ag.select(sum(\"UnitPrice\")).show()\n",
    "df_ag.select(sum_distinct(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb756f-4b86-4b38-8622-8ee61f58cc99",
   "metadata": {},
   "source": [
    "### avg\n",
    "Realiza la *media aritmética* de los valores de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1573811-11c9-430f-beda-1bdb17024f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df_ag.select(avg(\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ea6b2-b34d-419f-ba54-55adf43a5b23",
   "metadata": {},
   "source": [
    "### Otros parámetros estadísticos\n",
    "- varianza y desviación estándar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1fa0b6-3202-46a3-901d-a71e93aadedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "# Calculando varianza y desviación estándar sobre el total de la población\n",
    "df_ag.select(var_pop(\"Quantity\"), stddev_pop(\"Quantity\")).show()\n",
    "\n",
    "# Calculando varianza y desviación estándar sobre un muestreo de la población\n",
    "df_ag.select(var_samp(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a920e-9779-465a-81d5-237995645302",
   "metadata": {},
   "source": [
    "- oblicuidad y curtosis\n",
    "- covarianza y correlación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3281f-faf2-4eae-8399-93356df00984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df_ag.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()\n",
    "\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df_ag.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cca2f4e-ff70-4722-b8eb-974a860b220b",
   "metadata": {},
   "source": [
    "### GroupBy\n",
    "Para realizar los agrupamientos se necesitan dos fases:\n",
    "- **Agrupar**: Empleamos la función *groupBy* indicando la columna por la que queremos agrupar\n",
    "- **agregar**: Indicamos la función mediante la cual queremos agregar los resultados (en el ejemplo *count*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ded3e6-f7b4-4ca0-bb31-3aa7feaa0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ag.groupBy(\"InvoiceNo\").count().orderBy(\"count\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98f450-855e-4294-972f-a799d45d76da",
   "metadata": {},
   "source": [
    "También podemos agrupar empleando expresiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a3b35-a2a4-4555-8392-a8f8547e350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df_ag.groupBy(\"InvoiceNo\").agg(sum(\"Quantity\").alias(\"productos totales\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66c420-de3b-4bb7-bacd-4f3cc0e5ea45",
   "metadata": {},
   "source": [
    "### Agrupamiento con mapeo\n",
    "A veces puede ser útil expresar las transformaciones como series de *mapeos* en los cuales la *clave* es la columna y el *valor* es una función de agregación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e8fc1-329b-47f0-a166-7404c4412f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df_ag.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"), expr(\"stddev_pop(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf74077c-7626-4e1e-9664-88ca20d30364",
   "metadata": {},
   "source": [
    "## Agregaciones sobre tipos complejos\n",
    "Spark permite agregar valores no numéricos:\n",
    "- *collect_list*: Recolecta todos los valores de una determinada columna\n",
    "- *collect_set*: Recolecta todos los valores únicos de una determinada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8795e4-659c-4223-a06a-78189ad48cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set,collect_list\n",
    "df_ag.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba27a3-5f3d-40af-9fb5-1dec06b44594",
   "metadata": {},
   "source": [
    "# SQL\n",
    "Desde la versión 2.6 se pueden realizar consultas en SQL sobre los Dataframes empleando la función spark.sql(). Para poder hacerlo es necesario crear una vista temporal con la función createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f2e37-5558-4cc1-bdfc-196a428d07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.createOrReplaceTempView(\"vuelos\")\n",
    "spark.sql(\"SELECT * FROM vuelos WHERE count = 1\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8350faf-3056-424e-87b0-f2a8ba9b151f",
   "metadata": {},
   "source": [
    "## Exportar a pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc9756-433c-4edf-b71e-95a025ad3f92",
   "metadata": {},
   "source": [
    "Podemos pasar un Dataframe de Spark a DataFrame de Pandas empleando el método *toPandas()*. Primero, recordamos como podemos generar un DF con una estructura específica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2420e-aada-4499-92b9-ea7bb603a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#DataFrame API\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Definir el esquema\n",
    "\n",
    "schema = StructType([\n",
    "\n",
    "    StructField(\"InvoiceNo\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "\n",
    "    StructField(\"UnitPrice\", DoubleType(), True),\n",
    "\n",
    "    StructField(\"CustomerID\", IntegerType(), True),\n",
    "\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Leer el archivo CSV con el esquema especificado\n",
    "\n",
    "df_struct = spark.read.csv(\"data/retail-data/all/online-retail-dataset.csv\", header=True, schema=schema)\n",
    "df_struct.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1f1c5-48c5-4908-bbb0-b8e6f2b62378",
   "metadata": {},
   "source": [
    " Una vez tengamos el DF preparado, lo convertimos en un DataFrame de Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc6675-d94c-422b-be09-e83436e32f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_struct.groupBy(\"Country\").sum(\"Quantity\")\n",
    "df2.show()\n",
    "df2_pandas = df2.toPandas()\n",
    "display (df2_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155098c6-f866-44a3-a248-54e41a7eb8f3",
   "metadata": {},
   "source": [
    "Esto nos puede permitir generar gráficos de forma sencilla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d61aab-4569-494e-8091-db19528c77e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.ticklabel_format(useOffset=False, style=\"plain\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(x=\"sum(Quantity)\", y=\"Country\", data=df2_pandas).set_title(\"Productos vendidos por país\")\n",
    "plt.xlabel(\"Productos Vendidos\")\n",
    "plt.ylabel(\"Países\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87651ae1-48f2-4b7a-b514-3b03d8c75fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
