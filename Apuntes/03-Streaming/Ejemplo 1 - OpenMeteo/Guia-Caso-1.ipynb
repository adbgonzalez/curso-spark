{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baf23f8-79a5-4b7e-96d3-0790839a46fc",
   "metadata": {},
   "source": [
    "# Caso 1: Open-Meteo\n",
    "En este caso práctico vamos a obtener datos meteorológicos desde una API pública (Open-Meteo). El proyecto tendrá los siguientes componentes:\n",
    "- Productor kafka. Programa Python que accederá a la API a través del módulo request y producirá los resultados a un topic que será accedido después por nuestro programa spark.\n",
    "- Programa spark. programa que recopilará los datos de kafka y realizará agregaciones por ventanas de tiempo. En función de adónde se envíen los resultados tendremos dos variantes:\n",
    "  - A consola.\n",
    "  - A archivo parquet.\n",
    "- Por último tendremos un programa echo en *streamlit* que mostrará de forma visual los datos recopilados.\n",
    "## productor_open_meteo.py\n",
    "Necesitamos las siguientes importaciones:\n",
    "- **KafkaProducer** del módulo **kafka**.\n",
    "- **dumps** del paquete **json**-\n",
    "- **requests**: Para realizar peticiones a la API.\n",
    "- **time**.\n",
    "- **datetime**.\n",
    "\n",
    "Los pasos a seguir serán los siguientes:\n",
    "1. Crear un productor kafka.\n",
    "2. Hacer una llamada a la API de Open-Meteo.\n",
    "3. Transformar los datos a json y formatearlos.\n",
    "4. Usar el método send() de la clase *KafkaProducer* para escribir los datos a un topic.\n",
    "\n",
    "Lo primero será crear el productor kafka indicando el **bootstrap-server** y la codificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad1ed2-bcb8-4867-9e5d-ceb944c45199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka-1:9092'],\n",
    "    value_serializer=lambda x: dumps(x).encode('utf-8')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cfef2f-e8ca-4993-bee3-d948bbe315c6",
   "metadata": {},
   "source": [
    "Vamos a emplear la API de Open-Meteo. El primer paso sería consultar la [documentación oficial](https://open-meteo.com/en/docs). En la sección **API Documentation* vemos un listado de los parámetros disponibles, ente ellos *latitude* y *longitude* que son obligatorios.\n",
    "\n",
    "Vemos que las variables están distribuidas en varios gurpos: daily weather, hourly weather, current weather... Nos interesa este último.\n",
    "\n",
    "Para comunicarnos con la API vamos a emplear el módulo **requests**. Especificamos la url de la api y los parámetros (pares clave-valor) y llamamos al método **get()**. Convertimos la respuesta a *json* para explorar su estructura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8535227-74ce-43d5-a1af-4223d20e4b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"latitude\": 42.3125,\n",
      "  \"longitude\": -7.875,\n",
      "  \"generationtime_ms\": 0.046372413635253906,\n",
      "  \"utc_offset_seconds\": 0,\n",
      "  \"timezone\": \"GMT\",\n",
      "  \"timezone_abbreviation\": \"GMT\",\n",
      "  \"elevation\": 147.0,\n",
      "  \"current_weather_units\": {\n",
      "    \"time\": \"iso8601\",\n",
      "    \"interval\": \"seconds\",\n",
      "    \"temperature\": \"\\u00b0C\",\n",
      "    \"windspeed\": \"km/h\",\n",
      "    \"winddirection\": \"\\u00b0\",\n",
      "    \"is_day\": \"\",\n",
      "    \"weathercode\": \"wmo code\"\n",
      "  },\n",
      "  \"current_weather\": {\n",
      "    \"time\": \"2025-05-05T14:30\",\n",
      "    \"interval\": 900,\n",
      "    \"temperature\": 19.0,\n",
      "    \"windspeed\": 15.7,\n",
      "    \"winddirection\": 11,\n",
      "    \"is_day\": 1,\n",
      "    \"weathercode\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "params = {\n",
    "    \"latitude\": 42.33669,        # Coordenadas de Vigo\n",
    "    \"longitude\": -7.86407,\n",
    "    \"current_weather\": \"true\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "if response.ok:\n",
    "    data = response.json()\n",
    "    print(json.dumps(data, indent=2))\n",
    "else:\n",
    "    print(\"Erro:\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35ed4e-1d6b-4d8e-9247-aecc3710da3d",
   "metadata": {},
   "source": [
    "Una vez tenemos la respuesta el siguiente paso es obtener los datos que nos interesan, en nuestro caso *current_weather*. Le añadimos el nombre de la ciudad y un timestamp normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5171986-7de2-448b-a691-c7279763b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': '2025-05-05T14:30', 'interval': 900, 'temperature': 19.0, 'windspeed': 15.7, 'winddirection': 11, 'is_day': 1, 'weathercode': 3, 'city': 'Ourense', 'timestamp': '2025-05-05T14:43:19.547496'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from datetime import datetime\n",
    "data = response.json()[\"current_weather\"]\n",
    "\n",
    "data[\"city\"]=\"Ourense\"\n",
    "data[\"timestamp\"] = datetime.utcnow().isoformat()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8164ff-3c9d-47b7-8b31-1a27e395816e",
   "metadata": {},
   "source": [
    "Una vez tengamos los datos en el formato que queremos almacenados en una variable (en el ejemplo *data*) los escribimos al topic indicado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fe3d7-6072-444a-bf3a-45b5c8dd3243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "#...\n",
    "\n",
    "producer.send(\"topic\",data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbf8e3-7b5d-44ad-bd23-7c62b134589f",
   "metadata": {},
   "source": [
    "Se puede hacer lo mismo con varias localizaciones empleando un bucle.\n",
    "## Programa Spark\n",
    "La estructura del programa *spark* es muy similar a los notebooks vistos previamente:\n",
    "1. Inicializamos la sesión spark: Tenos que incluir los paquetes necesarios para usar *Kafka*.\n",
    "2. Definimos el esquema del DataFrame inicial.\n",
    "3. Definimos el flujo de entrada. En este caso tenemos que indicar, entre otros, el **bootstrap-server** y el nombre del **topic** del que se leerán los datos.\n",
    "4. Creamos un *DataFrame* nuevo transformando los datos iniciales. Hay que tener en cuenta las limitaciones para distintos tipos de agregaciones (especificadas en el documento 00 y en la [documentación oficial](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).\n",
    "5. Iniciamos el procesamiento, como siempre, indicando el destino y el modo.\n",
    "\n",
    "Ejemplo de inicialización de la sesión spark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284f0ee-66a2-4e74-aa8e-1c50bc2f7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenMeteoStreamingClean\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd9c7e-5c03-4ad2-96c7-935cc8f3b9f0",
   "metadata": {},
   "source": [
    "Ejemplo de definición del esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219da80-3522-45e6-9375-ea368cb40ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "schema = StructType() \\\n",
    "    .add(\"temperature\", DoubleType()) \\\n",
    "    .add(\"windspeed\", DoubleType()) \\\n",
    "    .add(\"winddirection\", DoubleType()) \\\n",
    "    .add(\"weathercode\", IntegerType()) \\\n",
    "    .add(\"time\", StringType()) \\\n",
    "    .add(\"city\", StringType()) \\\n",
    "    .add(\"local_timestamp\", StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2418e-803c-4b20-bd44-89121c17a3f9",
   "metadata": {},
   "source": [
    "Ejemplo De definición del flujo de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f11864-f933-42ad-a16c-a27a67673220",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "# Ejemplo Open-Meteo\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-1:9092\") \\\n",
    "    .option(\"subscribe\", \"open-meteo-weather\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Ejemplo general\n",
    "df_kafka = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"servidor-kafka\") \\\n",
    "    .option(\"subscribe\", \"nombre-topic\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f6198-e503-47f0-b0a3-9c527f364740",
   "metadata": {},
   "source": [
    "Ejemplo transformación de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff742ea9-c0f5-4400-b4b6-2106e2a0cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "\n",
    "# Transformación simple: filtrado y limpieza.\n",
    "df_parsed = df_kafka.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\n",
    "        col(\"data.city\"),\n",
    "        col(\"data.temperature\"),\n",
    "        col(\"data.windspeed\"),\n",
    "        col(\"data.winddirection\"),\n",
    "        to_timestamp(col(\"data.local_timestamp\")).alias(\"event_time\")\n",
    "    )\n",
    "\n",
    "#Transformación adicional empleando windowing y watermarking:\n",
    "# Agrupación por ventana y ciudad\n",
    "df_grouped = df_parsed \\\n",
    "    .withWatermark(\"event_time\", \"2 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"city\")\n",
    "    ).agg(\n",
    "        avg(\"temperature\").alias(\"avg_temp\"),\n",
    "        avg(\"windspeed\").alias(\"avg_wind\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6c2bb-3076-4387-b899-546024da1969",
   "metadata": {},
   "source": [
    "Ejemplo de inicio del procesamiento en streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cc5b5-0a16-4ee6-bdb0-4d0ae9bbe1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "\n",
    "# Mostrar en consola\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a82a06-5898-44dc-bd4b-cf1fffee5506",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO: NO EJECUTAR\n",
    "\n",
    "# Almacenar en parquet:\n",
    "query=df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"hdfs://spark-master:9000/user/jovyan/weather_aggregated/\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://spark-master:9000/user/jovyan/checkpoint_weather_aggregated/\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d7366-cc69-4b5f-8644-7d71d9361687",
   "metadata": {},
   "source": [
    "En esta carpeta hay ejemplos de productor y de programa spark. Recordad que para que funcione al ejecutarlos con spark submit hay que añadir la siguiente opción: **--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76559dc8-34b6-4626-b645-318988cb410a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
